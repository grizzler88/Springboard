{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FantasyNFL_Capstone_Preprocessing.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMl9+wriq7EekCCtv8U34YI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/grizzler88/Springboard/blob/master/Capstone%20-%20Fantasy%20Draft%20Strategy/FantasyNFL_Capstone_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUp_-swjt-v4"
      },
      "source": [
        "# Capstone 1: Fantasy NFL (Pre-processing & Training Data Developmnet)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCBp8qMAuSGs"
      },
      "source": [
        "The next step for my Capstone project is to clean up the latest verion of my dataset to ensure it is ready for the Modelling stage of the project."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPqphUbWnh_h"
      },
      "source": [
        "## Import packages & load dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ypq-wMAxGSw"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsZghOnjwo2T"
      },
      "source": [
        "df = pd.read_csv('NFL_FantasyData_2015_2019_EDA.csv')\r\n",
        "#df.head()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vp18uLyYytTh"
      },
      "source": [
        "Dataset has unamed column 'Unnamed: 0' from import that is not of value and should be removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBlhCn-vx7uj"
      },
      "source": [
        "df = df.drop(columns='Unnamed: 0')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq_wACrYyN48"
      },
      "source": [
        "#df.head()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G4uJrNQy_ULT",
        "outputId": "00cac44f-bb2d-4718-e582-e537e2e0cb61"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19529, 34)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNYAKPZU2R77"
      },
      "source": [
        "Next, we will look see what data types are in the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6H7QoSlzPFn",
        "outputId": "b64b1e7d-aeb6-47c5-8980-4f6ff2747625"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19529 entries, 0 to 19528\n",
            "Data columns (total 34 columns):\n",
            " #   Column          Non-Null Count  Dtype  \n",
            "---  ------          --------------  -----  \n",
            " 0   POS             19529 non-null  object \n",
            " 1   TEAM            19529 non-null  object \n",
            " 2   OPP             19529 non-null  object \n",
            " 3   DATE            19529 non-null  object \n",
            " 4   SEASON          19529 non-null  int64  \n",
            " 5   WEEK            19529 non-null  object \n",
            " 6   MONTH           19529 non-null  object \n",
            " 7   DAY             19529 non-null  object \n",
            " 8   TIME            19529 non-null  object \n",
            " 9   PLAYER          19529 non-null  object \n",
            " 10  FAN_ACTUAL      19529 non-null  float64\n",
            " 11  HOME            19529 non-null  int64  \n",
            " 12  DOME            19529 non-null  int64  \n",
            " 13  GRASS           19529 non-null  int64  \n",
            " 14  WEEK_NUMBER     19529 non-null  int64  \n",
            " 15  FAN_AVG         19529 non-null  float64\n",
            " 16  RUSHATT_AVG     19529 non-null  float64\n",
            " 17  RUSHYDS_AVG     19529 non-null  float64\n",
            " 18  RUSHTD_AVG      19529 non-null  float64\n",
            " 19  TGTS_AVG        19529 non-null  float64\n",
            " 20  REC_AVG         19529 non-null  float64\n",
            " 21  RECYDS_AVG      19529 non-null  float64\n",
            " 22  RECTD_AVG       19529 non-null  float64\n",
            " 23  FUM_AVG         19529 non-null  float64\n",
            " 24  FUMLST_AVG      19529 non-null  float64\n",
            " 25  RUSHYDS100_AVG  19529 non-null  float64\n",
            " 26  RUSHYDS200_AVG  19529 non-null  float64\n",
            " 27  RECYDS100_AVG   19529 non-null  float64\n",
            " 28  RECYDS200_AVG   19529 non-null  float64\n",
            " 29  PTSFOR_AVG      19529 non-null  float64\n",
            " 30  PTSAGT_AVG      19529 non-null  float64\n",
            " 31  LOSS_AVG        19529 non-null  float64\n",
            " 32  WIN_AVG         19529 non-null  float64\n",
            " 33  TIE_AVG         19529 non-null  float64\n",
            "dtypes: float64(20), int64(5), object(9)\n",
            "memory usage: 5.1+ MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mor-LMRc3mvt"
      },
      "source": [
        "There are 9 objects or categorical variables that we will need to make numeric."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eT7lDGay3w6w"
      },
      "source": [
        "## Categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYr5lWoN6tcl"
      },
      "source": [
        "object_cols = list(df.columns[df.dtypes == np.object])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtHoTad68Col",
        "outputId": "5a8d41c1-6e4e-4ffe-d677-a498493b4342"
      },
      "source": [
        "for x in object_cols:\r\n",
        "  val = df[x].nunique()\r\n",
        "  print(x,' = ', val)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "POS  =  4\n",
            "TEAM  =  32\n",
            "OPP  =  32\n",
            "DATE  =  248\n",
            "WEEK  =  17\n",
            "MONTH  =  5\n",
            "DAY  =  4\n",
            "TIME  =  5\n",
            "PLAYER  =  890\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnJQypM48uY9"
      },
      "source": [
        "The review of the ojbect columns shows that including them all would create +1,200 new columns.\r\n",
        "\r\n",
        "For the moment, we will not look at the date fields of DATE, WEEK, MONTH, DAY, TIME.\r\n",
        "\r\n",
        "We will instead concentrate on the POS, TEAM, OPP, and PLAYER columns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILAv0Klb-Pga"
      },
      "source": [
        "dummy_POS = pd.get_dummies(df.POS, prefix='POS', drop_first=True)\r\n",
        "dummy_TEAM = pd.get_dummies(df.TEAM, prefix='TEAM', drop_first=True)\r\n",
        "dummy_OPP = pd.get_dummies(df.OPP, prefix='OPP', drop_first=True)\r\n",
        "dummy_PLAYER = pd.get_dummies(df.PLAYER, prefix='PLAYER', drop_first=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwzwF0hM_Wrs"
      },
      "source": [
        "df = pd.concat([df, dummy_POS, dummy_TEAM, dummy_OPP, dummy_PLAYER], axis=1).drop(columns=['POS', 'TEAM', 'OPP', 'PLAYER'])\r\n",
        "#df.head()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AeXkuZv4Dj0N",
        "outputId": "ca7189c5-cd36-4281-d778-bf6cd8a6616f"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19529, 984)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4suFeODoNi"
      },
      "source": [
        "## Date Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDrguaqTEXyq"
      },
      "source": [
        "There are a couple of different steps to transforming the date variables mentioned previously.\r\n",
        "\r\n",
        "First, we will remove the 'Week' column because this is a duplicate of the 'WEEK_NUMBER' field that is already numeric and ordered."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Z9eVnLSDk1b"
      },
      "source": [
        "df = df.drop(columns=['WEEK'])"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jC_wRrT5E8IL"
      },
      "source": [
        "Next, we will turn the 'MONTH' and 'DAY' columns into numbers. \r\n",
        "\r\n",
        "While these fields are oridinal (in sense we rank months in order of when they happen), I don't believe using this would be correct for this analysis. This would lead to January being considered 11 values different to December which I don't believe accurately reflects the situation. Instead, we want to see if playing in one month or day impacts a player's pefromance. Therefore, we will look to again use dummy value for each column to support our modelling attempts.\r\n",
        "\r\n",
        "We will also apply this approach to the 'TIME' variable as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WiQoXbEE2-h"
      },
      "source": [
        "dummy_MONTH = pd.get_dummies(df.MONTH, prefix='MONTH', drop_first=True)\r\n",
        "dummy_DAY = pd.get_dummies(df.DAY, prefix='DAY', drop_first=True)\r\n",
        "dummy_TIME = pd.get_dummies(df.TIME, prefix='TIME', drop_first=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pd7GVtFICyM"
      },
      "source": [
        "df = pd.concat([df, dummy_MONTH, dummy_DAY, dummy_TIME], axis=1).drop(columns=['MONTH', 'DAY', 'TIME'])\r\n",
        "#df.head()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dHY2u8UHJWro"
      },
      "source": [
        "Lastly we have the 'DATE' column.\r\n",
        "\r\n",
        "To convert this, what we will convert the date to UNIX timestamp. This will allow us to keep same number of columns and not need to add an additional 247 rows for each date, as well as preserve intervals between dates. The negative of this approach is that it removes some of the interpretability related to the variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZfAh6-9TKG8x",
        "outputId": "8699259b-5c5c-4ba1-f287-22a79a0fc3de"
      },
      "source": [
        "df['DATE'] = pd.to_datetime(df.DATE)\r\n",
        "df.info()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 19529 entries, 0 to 19528\n",
            "Columns: 991 entries, DATE to TIME_Noon\n",
            "dtypes: datetime64[ns](1), float64(20), int64(5), uint8(965)\n",
            "memory usage: 21.8 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1w0I1lpmyTYn"
      },
      "source": [
        "df['DATE_STAMP'] = df['DATE'].values.astype(np.int64) // 10 ** 9\r\n",
        "df.DATE_STAMP.value_counts()\r\n",
        "df = df.drop(columns='DATE')\r\n",
        "#df.head()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BA4_zXZE_MH3",
        "outputId": "a2b7d5bb-c4a4-446a-c9f5-79df92945faa"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(19529, 991)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OUsF85f8Wv0"
      },
      "source": [
        "Now that all the data is in numeric format, we will have to scale to ensure it has the correct distribution to support modelling. However, prior to this, we will need to split our data into Train and Test data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcCzcHqg8JWd"
      },
      "source": [
        "## Training and Test Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQHA5ZJa_ln3"
      },
      "source": [
        "First, I will import the train_test_split fuction from sklearn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0kntCA98Vy6"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxGzh1Wx_v0x"
      },
      "source": [
        "Next, I will breakout my data into independent and dependent variables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkgtmNEe-aWb"
      },
      "source": [
        "X = df.drop('FAN_ACTUAL', axis=1)\r\n",
        "y = df['FAN_ACTUAL']"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOQOTokG_-bx"
      },
      "source": [
        "Now, I will split my data into training and test data. Due to the high number of dimensions in the dataset, I am going to set my test zize at 20% - lower than the default option of 25%. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MlVnSQ690GA"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Uh0X6PNBefn"
      },
      "source": [
        "## Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW5JCGXIBzZJ"
      },
      "source": [
        "df_columns = list(df.columns)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "lkUb96nPBiVO",
        "outputId": "e61b6626-0745-40f5-d2e5-a8d49839de7f"
      },
      "source": [
        "'''\r\n",
        "for x in df_columns:\r\n",
        "  _ = plt.hist(df[x])\r\n",
        "  _ = plt.title(x)\r\n",
        "  plt.show()\r\n",
        "'''"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nfor x in df_columns:\\n  _ = plt.hist(df[x])\\n  _ = plt.title(x)\\n  plt.show()\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3QjdO8ugE6vt"
      },
      "source": [
        "Most columns do not have normal distribution so we will need to scale our data to ensure it can be modelled correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3WDmAoVGbz-"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQKDT8GJGlVw"
      },
      "source": [
        "scaler = StandardScaler()\r\n",
        "scaler.fit(X_train)\r\n",
        "X_train_scaled = scaler.transform(X_train)\r\n",
        "X_test_scaled = scaler.transform(X_test)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ_4KcCVICHY"
      },
      "source": [
        "#pd.DataFrame(X_train_scaled, columns=list(X.columns))"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGol3qI4ImZM"
      },
      "source": [
        "#pd.DataFrame(X_test_scaled, columns=list(X.columns))"
      ],
      "execution_count": 26,
      "outputs": []
    }
  ]
}